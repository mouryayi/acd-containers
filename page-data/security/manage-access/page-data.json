{"componentChunkName":"component---src-pages-security-manage-access-md","path":"/security/manage-access/","result":{"pageContext":{"frontmatter":{"title":"Manage Access","excerpt":"Managing Access to IBM ACD.","categories":"security","slug":"manage-access","toc":true},"relativePagePath":"/security/manage-access.md","titleType":"page","MdxNode":{"id":"8f9f27a3-e2ec-556d-add4-6134400ce683","children":[],"parent":"1559707f-a471-5098-8e64-491dc819f993","internal":{"content":"---\ntitle: \"Manage Access\"\nexcerpt: \"Managing Access to IBM ACD.\"\ncategories: security\nslug: manage-access\ntoc: true\n---\n\n\n## Managing Access to ACD\n\nIf you have applications that run outside of the cluster and want to provide secure access to the ACD service in the cluster you can use the Openshift provided OAuth service with a [proxy](https://github.com/openshift/oauth-proxy) and a service account to do RBAC access to the service.\nIn the example below we'll use the latest version of the openshift oauth proxy.  See [instructions here](https://catalog.redhat.com/software/containers/openshift4/ose-oauth-proxy/5cdb2133bed8bd5717d5ae64?container-tabs=gti) for how to pull this image.\n\n1. Create a project/namespace for the proxy to run in (the examples below used ibm-wh-acd-oauth-proxy)\n\n    ```\n    oc create namespace ibm-wh-acd-oauth-proxy\n    ```\n\n1. Download the yaml below and save it as ibm-wh-acd-oauth-proxy.yaml.  Edit it with these changes:\n   * `namespace: ibm-wh-acd-oauth-proxy` to the namespace you created in 1.\n   * In the args section on this line `--upstream=https://ibm-wh-acd-acd.<namespace>.svc`  - change <namespace\\> to the namespace your acd instance is running in (the target service)\n   * On this line `--openshift-delegate-urls={\"/\":{\"resource\":\"services\",\"verb\":\"get\",\"namespace\":<namespace>}}`- change <namespace\\> again to match your target acd namespace.\n\n   ```yaml ibm-wh-acd-oauth-proxy.yaml\n    kind: List\n    apiVersion: v1\n    items:\n    # Create a proxy service account and ensure it will use the route \"proxy\"\n    - apiVersion: v1\n      kind: ServiceAccount\n      metadata:\n        name: proxy\n        annotations:\n          serviceaccounts.openshift.io/oauth-redirectreference.primary: '{\"kind\":\"OAuthRedirectReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"Route\",\"name\":\"proxy\"}}'\n    # Create a secure connection to the proxy via a route\n    - apiVersion: route.openshift.io/v1\n      kind: Route\n      metadata:\n        name: proxy\n      spec:\n        to:\n          kind: Service\n          name: proxy\n        tls:\n          termination: Reencrypt\n    - apiVersion: v1\n      kind: Service\n      metadata:\n        name: proxy\n        annotations:\n          service.alpha.openshift.io/serving-cert-secret-name: proxy-tls\n      spec:\n        ports:\n        - name: proxy\n          port: 443\n          targetPort: 8443\n        selector:\n          app: proxy\n    # Launch a proxy as a deployment\n    # use           - --set-xauthrequest=true option to send back the user info in the response to nginx or client\n    # see https://catalog.redhat.com/software/containers/openshift4/ose-oauth-proxy/5cdb2133bed8bd5717d5ae64?container-tabs=gti\n    - apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: proxy\n      spec:\n        replicas: 1\n        selector:\n          matchLabels:\n            app: proxy\n        template:\n          metadata:\n            labels:\n              app: proxy\n          spec:\n            serviceAccountName: proxy\n            containers:\n            - name: oauth-proxy\n              image: registry.redhat.io/openshift4/ose-oauth-proxy:v4.8\n              imagePullPolicy: IfNotPresent\n              ports:\n              - containerPort: 8443\n                name: public\n              args:\n              - --https-address=:8443\n              - --provider=openshift\n              - --request-logging=true\n              - --openshift-service-account=proxy\n              - --upstream=https://ibm-wh-acd-acd.<namespace>.svc\n              - --openshift-delegate-urls={\"/\":{\"resource\":\"services\",\"verb\":\"get\",\"namespace\":\"<namespace>\"}}\n              - --ssl-insecure-skip-verify=false\n              - --upstream-ca=/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt\n              - --tls-cert=/etc/tls/private/tls.crt\n              - --tls-key=/etc/tls/private/tls.key\n              - --cookie-secret=SECRET\n              - --footer=-\n              volumeMounts:\n              - mountPath: /etc/tls/private\n                name: proxy-tls\n            volumes:\n            - name: proxy-tls\n              secret:\n                secretName: proxy-tls\n   ```\n\n1. Run `oc project ibm-wh-acd-oauth-proxy` (change to your project used in  step 1 above).\n1. `oc create -f ibm-wh-acd-oauth-proxy.yaml`\n   * This should create a deployment for the proxy, a service to that deployment, a route to that service and a proxy service account all in your new project that was created in step 1.\n1. Because we are using the `--openshift-delegate-urls` option of the proxy we need to give the service account that checks the token permission to create a tokenreviews at the cluster level.  We'll do this by binding the proxy service account to the predefined `system:auth-delegator` cluster role.\n   * `oc adm policy add-cluster-role-to-user system:auth-delegator -z proxy -n ibm-wh-acd-oauth-proxy` (change project name as needed)\n1. The delegate-urls above says it will check the user/token coming in has the ability to do a get on the services in the target namespace as what will check to ensure the service account has access to the target acd instance.   We need to create a namespace-scoped role on the target acd project and grant access to the service account that will be used to call acd.  To start, create a role (named serviceview) on the namespace where acd is running and bind that role to the service account created above. Change <namespace\\> to your acd namespace and ibm-wh-acd-oauth-proxy to the namespace/project created in step 1 where the proxy runs.\n    * `oc create role serviceview --verb=get --resource=service -n <namespace>`\n    * `oc adm policy add-role-to-user serviceview system:serviceaccount:ibm-wh-acd-oauth-proxy:proxy --role-namespace=<namespace> -n <namespace>`\n1. Create a token to use on the service account\n   * `oc serviceaccounts new-token proxy -n ibm-wh-acd-oauth-proxy`  (change the project name as needed).  Copy the token returned (not this is also stored in a secret named proxy-token-nnnnn in the target project and when you delete that it will remove the token from the account after a bit. You can add additional tokens and remove the secret to rotate your tokens for your app.\n1. Use that token as a bearer token to call acd through the proxy route passing the bearer token on the Authorization header (eg:\n`curl -X GET -H \"Authorization: Bearer eyJ....z9g\" -k \"https://proxy-ibm-wh-acd-oauth-proxy.apps.youserver.com/services/clinical_data_annotator/api/v1/flows?version=2021-05-18\"`\n\nThe delegate-urls parms says for which paths it should forward and what the caller needs to have access to allow it.  In this example we forward all calls ('/') if the caller has `get` access to the services in the target acd namespace  (which we created the role and granted that permission the service account).  More info on openshift RBAC access is here - https://docs.openshift.com/container-platform/4.7/authentication/using-rbac.html\nNote here we have the proxy service account doing double duty - it is used to run the proxy service and do the token review (so it needed that cluster role binding) and we have it as the service account that is calling into ACD and bound it to that serviceview role to allow it.  In practice you have many service accounts and use their tokens to act as different acd tenants.\nAlso note if your application runs in the cluster you may want to consider using [bound service access tokens](https://docs.openshift.com/container-platform/4.7/authentication/bound-service-account-tokens.html) to have the tokens dynamically generated and rotated by the kubelet using the [TokenRequest api](https://jpweber.io/blog/a-look-at-tokenrequest-api/).\n\nMore options and details for the proxy are available at [OpenShift OAuth Proxy](https://github.com/openshift/oauth-proxy#openshift-oauth-proxy).\n\nInformation on troubleshooting the OAuth Proxy is found at [Troubleshooting the OAuth Proxy](/troubleshooting/troubleshooting-the-oauth-proxy/).\n\n### Multitenancy with ACD\n\nACD is stateless as far as the text it analyzes and returns however it does store configuration data through cartridges and flows and profiles and associated configuration artifacts.  You can use a single ACD instance with multiple 'tenants' which provides each tenant its own configuration storage area.  In order to use multitenancy you must use a security proxy as described above and use a different service account for each tenant. That provides security and passes back the tenant through a header to ACD but ACD needs to be configured to honor that header (it will use a defaultTenant as a tenant value for all calls otherwise).  In the ACD deployment (operand) when you create the instance you can set the tenantHeader value in the custom resource definition to specify the header to use as the tenant.  With the OAuth proxy above use the value of `X-Forwarded-User` (which the proxy sets to the service account name).  In the ACD custom resource definition this is `tenantHeader: X-Forwarded-User`.  You can edit or patch your instance to change or set it at create time.\n\n### Network Policies with ACD\n\nA set of Network Policies are created in the target ACD name space during the installation process. These Network Policies will only allow network traffic to flow from the ACD macro service to its back-end micro services within the same namespace.  The ACD macro service will also accept incoming connections from any namespace to port 9443 (via the ACD Service object) such as a Route ingress pod, Proxy pod, or other cluster application pod.\n\nYou can disable the Network Policy creation from the web console by setting `Network policy enabled` to false.  When installing from the command line, set the `networkPolicy.enabled` property to false.  This will remove any networking restrictions in the ACD namespace.\n\nYou can further enhance security by only allowing the ACD macro service to accept network traffic from a specific source namespace and/or its pods.  This can be done during instance creation in the web console by using the YAML View and adding network policy ingress selectors definitions at `spec.networkPolicy.ingress.fromSelectors`.  From the command line, set `networkPolicy.ingress.fromSelectors`.\n\n**Note:** Creating Network Policy definitions is an advanced topic and requires a good understanding of Network Policies configurations. More information can be found at [Kubernetes Network Policy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)\n\nSteps:\n\n1. Use an existing kubernetes label or add one to the source's namespace or deployment descriptor.  This label is used in the network policy to determine which incoming network traffic is allowed into the macro service.\n1. Modify the ACD instance yaml configuration (CSV) from the web console by adding a yaml block to the fromSelectors object.  There are two types of selectors, `namespaceSelectors` and `podSelectors`.  Choose one or both depending on the scope of restriction you want.  These selectors will get added to the networkpolicy-acd-macroservice.yaml NetworkPolicy definition in the ACD namespace.\n\nExample:\n\n```\nspec:\n  networkPolicy:\n    enabled: true\n    ingress:\n      fromSelectors:\n      - namespaceSelector:\n          matchLabels:\n            <label name>: <label value>\n        podSelector:\n          matchLabels:\n            <label name>: <label value>\n\n# Note that the use of the hyphen (-) on the selectors determines if this restriction is an AND or OR rule.  In the above example, both the namespace and pod selectors have to match since they are in the same array.\n```\n\nEgress\n\nThere are no egress Network Policies defined for the ACD namespace by default.  All pods within the ACD namespace can send data outside of the namespace including to the internet unless you have other restrictions in place.  It is also possible to restrict egress traffic from within the ACD namespace using Network Policy egress rules.\n","type":"Mdx","contentDigest":"f3ce03fc531fef526ad9b50356c597f9","owner":"gatsby-plugin-mdx","counter":225},"frontmatter":{"title":"Manage Access","excerpt":"Managing Access to IBM ACD.","categories":"security","slug":"manage-access","toc":true},"excerpt":"Managing Access to IBM ACD.","exports":{},"rawBody":"---\ntitle: \"Manage Access\"\nexcerpt: \"Managing Access to IBM ACD.\"\ncategories: security\nslug: manage-access\ntoc: true\n---\n\n\n## Managing Access to ACD\n\nIf you have applications that run outside of the cluster and want to provide secure access to the ACD service in the cluster you can use the Openshift provided OAuth service with a [proxy](https://github.com/openshift/oauth-proxy) and a service account to do RBAC access to the service.\nIn the example below we'll use the latest version of the openshift oauth proxy.  See [instructions here](https://catalog.redhat.com/software/containers/openshift4/ose-oauth-proxy/5cdb2133bed8bd5717d5ae64?container-tabs=gti) for how to pull this image.\n\n1. Create a project/namespace for the proxy to run in (the examples below used ibm-wh-acd-oauth-proxy)\n\n    ```\n    oc create namespace ibm-wh-acd-oauth-proxy\n    ```\n\n1. Download the yaml below and save it as ibm-wh-acd-oauth-proxy.yaml.  Edit it with these changes:\n   * `namespace: ibm-wh-acd-oauth-proxy` to the namespace you created in 1.\n   * In the args section on this line `--upstream=https://ibm-wh-acd-acd.<namespace>.svc`  - change <namespace\\> to the namespace your acd instance is running in (the target service)\n   * On this line `--openshift-delegate-urls={\"/\":{\"resource\":\"services\",\"verb\":\"get\",\"namespace\":<namespace>}}`- change <namespace\\> again to match your target acd namespace.\n\n   ```yaml ibm-wh-acd-oauth-proxy.yaml\n    kind: List\n    apiVersion: v1\n    items:\n    # Create a proxy service account and ensure it will use the route \"proxy\"\n    - apiVersion: v1\n      kind: ServiceAccount\n      metadata:\n        name: proxy\n        annotations:\n          serviceaccounts.openshift.io/oauth-redirectreference.primary: '{\"kind\":\"OAuthRedirectReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"Route\",\"name\":\"proxy\"}}'\n    # Create a secure connection to the proxy via a route\n    - apiVersion: route.openshift.io/v1\n      kind: Route\n      metadata:\n        name: proxy\n      spec:\n        to:\n          kind: Service\n          name: proxy\n        tls:\n          termination: Reencrypt\n    - apiVersion: v1\n      kind: Service\n      metadata:\n        name: proxy\n        annotations:\n          service.alpha.openshift.io/serving-cert-secret-name: proxy-tls\n      spec:\n        ports:\n        - name: proxy\n          port: 443\n          targetPort: 8443\n        selector:\n          app: proxy\n    # Launch a proxy as a deployment\n    # use           - --set-xauthrequest=true option to send back the user info in the response to nginx or client\n    # see https://catalog.redhat.com/software/containers/openshift4/ose-oauth-proxy/5cdb2133bed8bd5717d5ae64?container-tabs=gti\n    - apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: proxy\n      spec:\n        replicas: 1\n        selector:\n          matchLabels:\n            app: proxy\n        template:\n          metadata:\n            labels:\n              app: proxy\n          spec:\n            serviceAccountName: proxy\n            containers:\n            - name: oauth-proxy\n              image: registry.redhat.io/openshift4/ose-oauth-proxy:v4.8\n              imagePullPolicy: IfNotPresent\n              ports:\n              - containerPort: 8443\n                name: public\n              args:\n              - --https-address=:8443\n              - --provider=openshift\n              - --request-logging=true\n              - --openshift-service-account=proxy\n              - --upstream=https://ibm-wh-acd-acd.<namespace>.svc\n              - --openshift-delegate-urls={\"/\":{\"resource\":\"services\",\"verb\":\"get\",\"namespace\":\"<namespace>\"}}\n              - --ssl-insecure-skip-verify=false\n              - --upstream-ca=/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt\n              - --tls-cert=/etc/tls/private/tls.crt\n              - --tls-key=/etc/tls/private/tls.key\n              - --cookie-secret=SECRET\n              - --footer=-\n              volumeMounts:\n              - mountPath: /etc/tls/private\n                name: proxy-tls\n            volumes:\n            - name: proxy-tls\n              secret:\n                secretName: proxy-tls\n   ```\n\n1. Run `oc project ibm-wh-acd-oauth-proxy` (change to your project used in  step 1 above).\n1. `oc create -f ibm-wh-acd-oauth-proxy.yaml`\n   * This should create a deployment for the proxy, a service to that deployment, a route to that service and a proxy service account all in your new project that was created in step 1.\n1. Because we are using the `--openshift-delegate-urls` option of the proxy we need to give the service account that checks the token permission to create a tokenreviews at the cluster level.  We'll do this by binding the proxy service account to the predefined `system:auth-delegator` cluster role.\n   * `oc adm policy add-cluster-role-to-user system:auth-delegator -z proxy -n ibm-wh-acd-oauth-proxy` (change project name as needed)\n1. The delegate-urls above says it will check the user/token coming in has the ability to do a get on the services in the target namespace as what will check to ensure the service account has access to the target acd instance.   We need to create a namespace-scoped role on the target acd project and grant access to the service account that will be used to call acd.  To start, create a role (named serviceview) on the namespace where acd is running and bind that role to the service account created above. Change <namespace\\> to your acd namespace and ibm-wh-acd-oauth-proxy to the namespace/project created in step 1 where the proxy runs.\n    * `oc create role serviceview --verb=get --resource=service -n <namespace>`\n    * `oc adm policy add-role-to-user serviceview system:serviceaccount:ibm-wh-acd-oauth-proxy:proxy --role-namespace=<namespace> -n <namespace>`\n1. Create a token to use on the service account\n   * `oc serviceaccounts new-token proxy -n ibm-wh-acd-oauth-proxy`  (change the project name as needed).  Copy the token returned (not this is also stored in a secret named proxy-token-nnnnn in the target project and when you delete that it will remove the token from the account after a bit. You can add additional tokens and remove the secret to rotate your tokens for your app.\n1. Use that token as a bearer token to call acd through the proxy route passing the bearer token on the Authorization header (eg:\n`curl -X GET -H \"Authorization: Bearer eyJ....z9g\" -k \"https://proxy-ibm-wh-acd-oauth-proxy.apps.youserver.com/services/clinical_data_annotator/api/v1/flows?version=2021-05-18\"`\n\nThe delegate-urls parms says for which paths it should forward and what the caller needs to have access to allow it.  In this example we forward all calls ('/') if the caller has `get` access to the services in the target acd namespace  (which we created the role and granted that permission the service account).  More info on openshift RBAC access is here - https://docs.openshift.com/container-platform/4.7/authentication/using-rbac.html\nNote here we have the proxy service account doing double duty - it is used to run the proxy service and do the token review (so it needed that cluster role binding) and we have it as the service account that is calling into ACD and bound it to that serviceview role to allow it.  In practice you have many service accounts and use their tokens to act as different acd tenants.\nAlso note if your application runs in the cluster you may want to consider using [bound service access tokens](https://docs.openshift.com/container-platform/4.7/authentication/bound-service-account-tokens.html) to have the tokens dynamically generated and rotated by the kubelet using the [TokenRequest api](https://jpweber.io/blog/a-look-at-tokenrequest-api/).\n\nMore options and details for the proxy are available at [OpenShift OAuth Proxy](https://github.com/openshift/oauth-proxy#openshift-oauth-proxy).\n\nInformation on troubleshooting the OAuth Proxy is found at [Troubleshooting the OAuth Proxy](/troubleshooting/troubleshooting-the-oauth-proxy/).\n\n### Multitenancy with ACD\n\nACD is stateless as far as the text it analyzes and returns however it does store configuration data through cartridges and flows and profiles and associated configuration artifacts.  You can use a single ACD instance with multiple 'tenants' which provides each tenant its own configuration storage area.  In order to use multitenancy you must use a security proxy as described above and use a different service account for each tenant. That provides security and passes back the tenant through a header to ACD but ACD needs to be configured to honor that header (it will use a defaultTenant as a tenant value for all calls otherwise).  In the ACD deployment (operand) when you create the instance you can set the tenantHeader value in the custom resource definition to specify the header to use as the tenant.  With the OAuth proxy above use the value of `X-Forwarded-User` (which the proxy sets to the service account name).  In the ACD custom resource definition this is `tenantHeader: X-Forwarded-User`.  You can edit or patch your instance to change or set it at create time.\n\n### Network Policies with ACD\n\nA set of Network Policies are created in the target ACD name space during the installation process. These Network Policies will only allow network traffic to flow from the ACD macro service to its back-end micro services within the same namespace.  The ACD macro service will also accept incoming connections from any namespace to port 9443 (via the ACD Service object) such as a Route ingress pod, Proxy pod, or other cluster application pod.\n\nYou can disable the Network Policy creation from the web console by setting `Network policy enabled` to false.  When installing from the command line, set the `networkPolicy.enabled` property to false.  This will remove any networking restrictions in the ACD namespace.\n\nYou can further enhance security by only allowing the ACD macro service to accept network traffic from a specific source namespace and/or its pods.  This can be done during instance creation in the web console by using the YAML View and adding network policy ingress selectors definitions at `spec.networkPolicy.ingress.fromSelectors`.  From the command line, set `networkPolicy.ingress.fromSelectors`.\n\n**Note:** Creating Network Policy definitions is an advanced topic and requires a good understanding of Network Policies configurations. More information can be found at [Kubernetes Network Policy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)\n\nSteps:\n\n1. Use an existing kubernetes label or add one to the source's namespace or deployment descriptor.  This label is used in the network policy to determine which incoming network traffic is allowed into the macro service.\n1. Modify the ACD instance yaml configuration (CSV) from the web console by adding a yaml block to the fromSelectors object.  There are two types of selectors, `namespaceSelectors` and `podSelectors`.  Choose one or both depending on the scope of restriction you want.  These selectors will get added to the networkpolicy-acd-macroservice.yaml NetworkPolicy definition in the ACD namespace.\n\nExample:\n\n```\nspec:\n  networkPolicy:\n    enabled: true\n    ingress:\n      fromSelectors:\n      - namespaceSelector:\n          matchLabels:\n            <label name>: <label value>\n        podSelector:\n          matchLabels:\n            <label name>: <label value>\n\n# Note that the use of the hyphen (-) on the selectors determines if this restriction is an AND or OR rule.  In the above example, both the namespace and pod selectors have to match since they are in the same array.\n```\n\nEgress\n\nThere are no egress Network Policies defined for the ACD namespace by default.  All pods within the ACD namespace can send data outside of the namespace including to the internet unless you have other restrictions in place.  It is also possible to restrict egress traffic from within the ACD namespace using Network Policy egress rules.\n","fileAbsolutePath":"/home/travis/build/IBM/acd-containers/src/pages/security/manage-access.md"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}